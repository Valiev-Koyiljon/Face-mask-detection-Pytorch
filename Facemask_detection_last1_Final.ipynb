{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valiev-Koyiljon/Face-mask-detection-Pytorch/blob/main/Facemask_detection_last1_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld7UXiLgi9nk"
      },
      "source": [
        "#Facemask Detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRT0CJM9k0jl"
      },
      "source": [
        "\n",
        "Title: Real-time Face Mask Detection using Pre-trained ResNet Model\n",
        "\n",
        "Description:\n",
        "The project aims to detect whether a person is wearing a face mask or not in real-time using computer vision techniques and a pre-trained ResNet model. The system captures video frames from a webcam, applies face detection to locate faces in the frames, and then utilizes a deep learning model to classify each detected face as either wearing a mask or not.\n",
        "\n",
        "The project consists of two main parts:\n",
        "\n",
        "1. Training the Face Mask Detection Model:\n",
        "   - The project starts with training a face mask detection model using a pre-trained ResNet model.\n",
        "   - A dataset of 100 images is used for training the model, where each image is labeled as either having a person wearing a mask or not.\n",
        "   - The pre-trained ResNet model, specifically ResNet-50, is employed as a feature extractor, and the final fully connected layer is modified to have two output neurons representing mask and no-mask classes.\n",
        "   - The model is trained using the dataset, and the training process involves optimizing the model's parameters using an Adam optimizer and cross-entropy loss.\n",
        "   - After training, the model is saved to a file for later use during real-time face mask detection.\n",
        "\n",
        "2. Real-time Face Mask Detection:\n",
        "   - Once the model is trained and saved, the project focuses on real-time face mask detection using OpenCV.\n",
        "   - The system captures video frames from the webcam and applies face detection using the Haar cascade classifier provided by OpenCV.\n",
        "   - For each detected face, the model is utilized to classify whether the person is wearing a mask or not by passing the face through the trained model.\n",
        "   - The result is then displayed on the screen by drawing a rectangle around the face and adding a label indicating whether the person is wearing a mask or not.\n",
        "   - The process continues in real-time, detecting faces and updating the display accordingly.\n",
        "   - The system allows the user to exit the application by pressing the 'q' key.\n",
        "\n",
        "The project combines deep learning techniques with computer vision to address the important task of face mask detection. It can be applied in various settings, such as public spaces, workplaces, and healthcare facilities, to help ensure adherence to face mask policies and promote public health and safety."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning. Part I"
      ],
      "metadata": {
        "id": "0tToFcQuh1t6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbTn6pw8zOuO",
        "outputId": "b7de551c-f872-4f37-ced6-f4ac190c1427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwRQCBEyzqVB",
        "outputId": "c6fc522b-85de-4c9c-fa61-56491a37cee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "#  path to the ZIP file\n",
        "zip_file_path = \"/content/drive/MyDrive/Colab Notebooks/mask_images_ready.zip\"\n",
        "destination_folder = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)\n",
        "\n",
        "\n",
        "print(\"Folder extraction completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir_4iYKUO8jo"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages and libraries\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q4dlSH6BMNk"
      },
      "outputs": [],
      "source": [
        "# changing the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9yqjG37BOl9"
      },
      "outputs": [],
      "source": [
        "#  FaceMaskDataset class for training, validation, and test data\n",
        "class FaceMaskDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNMz5n10BX6D"
      },
      "outputs": [],
      "source": [
        "# FaceMaskDetector class\n",
        "class FaceMaskDetector:\n",
        "    def __init__(self, model_path):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self.load_model(model_path).to(self.device)\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        model = models.resnet50(pretrained=False)\n",
        "        num_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_features, 2)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def detect(self, frame):\n",
        "        frame = self.transform(frame).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            output = self.model(frame)\n",
        "        probabilities = torch.softmax(output, dim=1)[0]\n",
        "        mask_probability = probabilities[1].item()\n",
        "        no_mask_probability = probabilities[0].item()\n",
        "        label = \"Mask\" if mask_probability > no_mask_probability else \"No Mask\"\n",
        "        return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIUyRUYJBdaj"
      },
      "outputs": [],
      "source": [
        "# The training parameters\n",
        "train_data_dir = \"/content/drive/MyDrive/Colab Notebooks/mask_images_ready/train\"\n",
        "val_data_dir = \"/content/drive/MyDrive/Colab Notebooks/mask_images_ready/val\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Colab Notebooks/mask_images_ready/test\"\n",
        "model_save_path = \"/content/drive/MyDrive/Colab Notebooks/Final_FaceMask_detection_model.pt\"\n",
        "batch_size = 16\n",
        "num_epochs = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzCgrc3qBjbB"
      },
      "outputs": [],
      "source": [
        "# Load the training data\n",
        "train_image_paths = []\n",
        "train_labels = []\n",
        "\n",
        "for folder_name in os.listdir(train_data_dir):\n",
        "    folder_path = os.path.join(train_data_dir, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.endswith(\".png\"): \n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                train_image_paths.append(file_path)\n",
        "                train_labels.append(1 if folder_name == \"1\" else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg5NWFP9Bldu"
      },
      "outputs": [],
      "source": [
        "# Load the validation data\n",
        "val_image_paths = []\n",
        "val_labels = []\n",
        "\n",
        "for folder_name in os.listdir(val_data_dir):\n",
        "    folder_path = os.path.join(val_data_dir, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.endswith(\".png\"):\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                val_image_paths.append(file_path)\n",
        "                val_labels.append(1 if folder_name == \"1\" else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20jEDZs_BlI6"
      },
      "outputs": [],
      "source": [
        "# Load the test data\n",
        "test_image_paths = []\n",
        "test_labels = []\n",
        "\n",
        "for folder_name in os.listdir(test_data_dir):\n",
        "    folder_path = os.path.join(test_data_dir, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.endswith(\".png\"):\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                test_image_paths.append(file_path)\n",
        "                test_labels.append(1 if folder_name == \"1\" else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLL6meijB0Fr"
      },
      "outputs": [],
      "source": [
        "# FaceMaskDataset class for training, validation, and test data\n",
        "train_dataset = FaceMaskDataset(train_image_paths, train_labels, transform=transforms.ToTensor())\n",
        "val_dataset = FaceMaskDataset(val_image_paths, val_labels, transform=transforms.ToTensor())\n",
        "test_dataset = FaceMaskDataset(test_image_paths, test_labels, transform=transforms.ToTensor())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlObLVgUB1sk"
      },
      "outputs": [],
      "source": [
        "#  training, validation, and test data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLjJ5QJWB8Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e44970-f8d8-4356-db2b-9a1354cc5a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# FaceMaskModel class\n",
        "class FaceMaskModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceMaskModel, self).__init__()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        num_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "# FaceMaskModel class\n",
        "model = FaceMaskModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1tvcCZ7CIlC"
      },
      "outputs": [],
      "source": [
        "# the loss function, optimizer, and learning rate scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "\n",
        "# Move the model to the device\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DxA2T2CIOs",
        "outputId": "bb3fbba5-eb6f-4f8e-a356-b445f966a327",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "Train Loss: 0.1665 | Train Accuracy: 91.67%\n",
            "Val Loss: 3.8828 | Val Accuracy: 50.00%\n",
            "-------------------------------\n",
            "Epoch 2/12\n",
            "Train Loss: 0.0006 | Train Accuracy: 100.00%\n",
            "Val Loss: 5.3019 | Val Accuracy: 50.00%\n",
            "-------------------------------\n",
            "Epoch 3/12\n",
            "Train Loss: 0.0011 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.8417 | Val Accuracy: 92.86%\n",
            "-------------------------------\n",
            "Epoch 4/12\n",
            "Train Loss: 0.0000 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.4044 | Val Accuracy: 92.86%\n",
            "-------------------------------\n",
            "Epoch 5/12\n",
            "Train Loss: 0.0000 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.1364 | Val Accuracy: 92.86%\n",
            "-------------------------------\n",
            "Epoch 6/12\n",
            "Train Loss: 0.0002 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.0111 | Val Accuracy: 100.00%\n",
            "-------------------------------\n",
            "Epoch 7/12\n",
            "Train Loss: 0.0000 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.0001 | Val Accuracy: 100.00%\n",
            "-------------------------------\n",
            "Epoch 8/12\n",
            "Train Loss: 0.0000 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.0000 | Val Accuracy: 100.00%\n",
            "-------------------------------\n",
            "Epoch 9/12\n",
            "Train Loss: 0.0000 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.0000 | Val Accuracy: 100.00%\n",
            "-------------------------------\n",
            "Epoch 10/12\n",
            "Train Loss: 0.0000 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.0000 | Val Accuracy: 100.00%\n",
            "-------------------------------\n",
            "Epoch 11/12\n",
            "Train Loss: 0.0001 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.0000 | Val Accuracy: 100.00%\n",
            "-------------------------------\n",
            "Epoch 12/12\n",
            "Train Loss: 0.0001 | Train Accuracy: 100.00%\n",
            "Val Loss: 0.0000 | Val Accuracy: 100.00%\n",
            "-------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_accuracy = correct / total\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    # Model evaluation on the validation data\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_accuracy = val_correct / val_total\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy*100:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy*100:.2f}%\")\n",
        "    print(\"-------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmGBnKUACIDy",
        "outputId": "301fb2c6-59da-41e6-8b2f-28565773c5ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0000 | Test Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "\n",
        "\n",
        "# Model evaluation on test data\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "        test_loss += loss.item()\n",
        "\n",
        "test_accuracy = test_correct / test_total\n",
        "test_loss = test_loss / len(test_loader)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "# save the model one more time after checking good accuracy\n",
        "torch.save(model.state_dict(), model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zIDb6OJfjE"
      },
      "source": [
        "For downloading our model from google colab noteb, we will use the below codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Et-aV0ZNpNVm",
        "outputId": "232046ff-c534-4b13-893f-a71e721929e4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_45d30a17-3702-4837-979a-9efecbd1bebf\", \"Final_FaceMask_detection_model.pt\", 94375983)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "model_path = model_save_path\n",
        "\n",
        "files.download(model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8kJYijYPNLG"
      },
      "source": [
        "#Checking our model by uploading pictures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "79Ko1dWq1oj4",
        "outputId": "45919ba7-855e-43a9-9553-a5a79ddbc62f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0674280c-6684-40e6-b86b-9300bcc71245\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0674280c-6684-40e6-b86b-9300bcc71245\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving personwithmask.jpg to personwithmask (4).jpg\n",
            "Face mask detected with probability: 0.5502548217773438\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "class FaceMaskModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceMaskModel, self).__init__()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        num_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "# calling our model\n",
        "model = FaceMaskModel()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# the image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), \n",
        "    transforms.ToTensor(),  \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "\n",
        "# Getting the uploaded image file\n",
        "image_path = list(uploaded.keys())[0]\n",
        "image = Image.open(image_path)\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess the image\n",
        "input_tensor = transform(image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# face mask detection\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output = model(input_batch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class_probabilities = torch.softmax(output, dim=1)\n",
        "mask_probability = class_probabilities[0, 1].item() \n",
        "\n",
        "\n",
        "threshold = 0.5  #treshold\n",
        "if mask_probability > threshold:\n",
        "    print(\"Face mask detected with probability:\", mask_probability)\n",
        "else:\n",
        "    print(\"No face mask detected with probability:\", 1 - mask_probability)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cO7eXX5FgSbv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}